{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Training Logistic Regression Model on the Data**"
      ],
      "metadata": {
        "id": "SIc-7Cw1ZOMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Team Members: Mingyuan Ma, Shuyao Zhou, Yuanrui Zhu*"
      ],
      "metadata": {
        "id": "nTcL8_AIdKdF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re-wC0bqFZn8"
      },
      "source": [
        "L2-regularized logistic regression for binary or multiclass classification; trains a model (on `train.txt`), optimizes L2 regularization strength on `dev.txt`, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals and prints out the strongest coefficients for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---Introduction---"
      ],
      "metadata": {
        "id": "Isf8Vo3HdT4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a bigram + trigram model instead of BoW to conduct feature engineering. On top of that, we concluded approximately 50 common words which are strong signs of casualness and give features containing those words a higher weight. We also deal with emojis by converting them to words so they would be taken into account within the bigram model (instead of being deleted thus losing the predictive power). "
      ],
      "metadata": {
        "id": "2utQPwoec0hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---Code---"
      ],
      "metadata": {
        "id": "7M-o-1xJeJJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "import math\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have your folder of data on your Google drive account, you can connect that here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPQgJrWLFvKP",
        "outputId": "6a8a5f44-c031-4b6d-c0b9-1fa422b6e48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to the directory with your data\n",
        "directory=\"/content/drive/MyDrive/ap_data\"#/43"
      ],
      "metadata": {
        "id": "mjfXTlTyGJUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KuVSCSqlUX",
        "outputId": "2b855937-8fae-4d61-8af8-9477ab68b361",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMbXH2jCFZoA"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1].lstrip().rstrip()\n",
        "            text = cols[2]\n",
        "\n",
        "            X.append(text)\n",
        "            Y.append(label)\n",
        "\n",
        "    return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, trainX, trainY, devX, devY, testX, testY):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count=2\n",
        "        self.log_reg = None\n",
        "\n",
        "        self.trainY=trainY\n",
        "        self.devY=devY\n",
        "        self.testY=testY\n",
        "        \n",
        "        self.trainX = self.process(trainX, training=True)\n",
        "        self.devX = self.process(devX, training=False)\n",
        "        self.testX = self.process(testX, training=False)\n",
        "\n",
        "        self.predict = [] \n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append(feats)\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, X_data, training = False):\n",
        "        \n",
        "        data = self.featurize(X_data)\n",
        "        \n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        \n",
        "        for idx, feats in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "        \n",
        "        return X\n",
        "\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def train(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        best_dev_accuracy=0\n",
        "        best_model=None\n",
        "        for C in [0.1, 1, 10, 100]:\n",
        "            self.log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
        "            self.log_reg.fit(self.trainX, self.trainY)\n",
        "            training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "            development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "            if development_accuracy > best_dev_accuracy:\n",
        "                best_dev_accuracy=development_accuracy\n",
        "                best_model=self.log_reg\n",
        "\n",
        "            # print(\"C: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (C, training_accuracy, development_accuracy))\n",
        "\n",
        "        self.log_reg=best_model\n",
        "\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "        predict = self.log_reg.predict(self.testX)\n",
        "        self.predict = predict\n",
        "        return self.log_reg.score(self.testX, self.testY)\n",
        "        \n",
        "\n",
        "\n",
        "    def get_predict(self): \n",
        "        return self.predict\n",
        "\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (int(cat)-1, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (int(cat)-1, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (int(cat)-1, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtgwLFauFZoB"
      },
      "outputs": [],
      "source": [
        "def binary_bow_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        feats[word]=1\n",
        "            \n",
        "    return feats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot\n",
        "# please run this for the first time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4y5jjoVL01s",
        "outputId": "b6b60eaa-d495-455b-f2e2-94d6572542ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 2.1 kB/s \n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
        "import re\n",
        "\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMOJI:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "    return text\n",
        "\n",
        "\n",
        "def weighted_convoluation_featurize(text):\n",
        "\n",
        "    feats = {}\n",
        "    text = convert_emojis(text)\n",
        "    \n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # feats[\"smaller-than-10\"] = 1 if len(words) <=10 else 0\n",
        "    # feats[\"smaller-than-50\"] = 1 if len(words) <=50 and len(words) >10 else 0\n",
        "    # feats[\"larger-than-50\"] = 1 if len(words) >50 else 0\n",
        "\n",
        "    pointer1, pointer2 = 0, 0\n",
        "\n",
        "    casual = [\"oh\",\"cool\",\"!\", \"!!\", \"!!!\", \"tbh\", \"btw\", \"lmao\", \"asap\", \"wow\", \"somehow\",\n",
        "              \"awesome\", \":)\", \":(\", \": )\", \"...\", \"......\", \"fuck\", \"shit\", \"damn\", \"legit\", \"gotta\",\n",
        "              \"absolutely\", \"extremely\", \"definitely\", \"hate\", \"right\", \"dude\", \"idk\", \"Idk\"\n",
        "              \"could’ve\", \"how’s\", \"I’d\", \"let’s\", \"ma’am\", \"might’ve\", \"mustn’t\", \"gunna\"\n",
        "              \"needn’t\", \"not’ve\", \"o’cloc\", \"there’d\", \"there’d\", \"there’s\", \"they’d\", \"wrong\", \"subs\",\n",
        "              \"they’re\", \"we’d\", \"gonna\", \"wanna\", \"probably\", \"pitbull\", \"piss\", \"pisses\"\n",
        "              \"when’s\", \"where’d\", \"where’s\", \"where’ve\", \"who’ll\", \"who’s\", \"?\", \"smiling_face️\", \n",
        "              \"face_with_tears_of_joy\", \"smiling_face_with_sunglasses\", \"loudly_crying_face\", \"absolutely\",\n",
        "              \"sleeping_face\", \"grinning_face_with_smiling_eyes\", \"winking_face_with_tongue\", \"love\"\n",
        "              \"grinning_face\", \"grinning_squinting_face\", \"grinning_face_with_sweat\", \"face_blowing_a_kiss\",\n",
        "              \"smiling_face_with_heart-eyes\", \"persevering_face\", \"face_with_steam_from_nose\", \n",
        "              \"face_vomiting\", \"thumbs_up victory_hand️\"]\n",
        "\n",
        "    def getWeights(combo):\n",
        "      weight = 1\n",
        "      for cas in casual:\n",
        "        if cas in combo:\n",
        "          weight *= 1.2\n",
        "      return weight\n",
        "\n",
        "    for word in words:\n",
        "      word=word.lower()\n",
        "      feats[word]=getWeights(word)  \n",
        "    \n",
        "    if len(words) >= 2:\n",
        "      while pointer1 != len(words) - 1:\n",
        "        adjacent = words[pointer1 + 1]\n",
        "        feats[words[pointer1] + \" \" + adjacent] = 1\n",
        "        pointer1 += 1\n",
        "\n",
        "    if len(words) >= 3:\n",
        "      while pointer2 != len(words) - 2:\n",
        "        adjacent = words[pointer2 + 1]\n",
        "        far = words[pointer2 + 2]\n",
        "\n",
        "        combo2 = words[pointer2] + \" \" + adjacent + \" \"\n",
        "        combo3 = words[pointer2] + \" \" + adjacent + \" \" + far\n",
        "\n",
        "        feats[combo2] = getWeights(combo2)\n",
        "        feats[combo3] = getWeights(combo3)\n",
        "\n",
        "        pointer2 += 1\n",
        "\n",
        "\n",
        "    return feats"
      ],
      "metadata": {
        "id": "irZumT5x5wPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N914JaNxFZoC"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    critical_value=(1-significance_level)/2\n",
        "    z_alpha=-1*norm.ppf(critical_value)\n",
        "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
        "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAgCztpXFZoD"
      },
      "outputs": [],
      "source": [
        "def run(trainingFile, devFile, testFile):\n",
        "    trainX, trainY=load_data(trainingFile)\n",
        "    devX, devY=load_data(devFile)\n",
        "    testX, testY=load_data(testFile)\n",
        "    \n",
        "    simple_classifier = Classifier(weighted_convoluation_featurize, trainX, trainY, devX, devY, testX, testY)\n",
        "    simple_classifier.train()\n",
        "    accuracy=simple_classifier.test()\n",
        "\n",
        "    y_pred = simple_classifier.get_predict()\n",
        "   \n",
        "\n",
        "    lower, upper=confidence_intervals(accuracy, len(testY), .95)\n",
        "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
        "\n",
        "    print(\"The Strongest coefficients for each class are: \")\n",
        "    simple_classifier.printWeights()\n",
        "    return y_pred, testY, testX, trainY, trainX\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv5tsoWDFZoE",
        "outputId": "34efabda-74eb-40ee-c0bc-84a6a8781388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model: 0.575, 95% CIs: [0.506 0.644]\n",
            "\n",
            "The Strongest coefficients for each class are: \n",
            "0\t0.753\t!\n",
            "0\t0.409\ti\n",
            "0\t0.244\t?\n",
            "0\t0.232\t...\n",
            "0\t0.195\tme\n",
            "0\t0.170\tmy\n",
            "0\t0.169\t: )\n",
            "0\t0.161\t'm\n",
            "0\t0.154\twith\n",
            "0\t0.146\tna\n",
            "\n",
            "1\t0.236\tif\n",
            "1\t0.176\tIf you \n",
            "1\t0.176\tIf you\n",
            "1\t0.175\tr/52book\n",
            "1\t0.154\tliterature\n",
            "1\t0.151\tbookclub\n",
            "1\t0.147\tmaybe\n",
            "1\t0.144\tfound\n",
            "1\t0.140\tthen\n",
            "1\t0.139\thave\n",
            "\n",
            "2\t0.233\tas\n",
            "2\t0.228\tby\n",
            "2\t0.228\tbe\n",
            "2\t0.222\tr/truelit\n",
            "2\t0.210\t's\n",
            "2\t0.194\t.\n",
            "2\t0.182\tare\n",
            "2\t0.179\ton\n",
            "2\t0.167\t/r/truelit\n",
            "2\t0.164\t/r/askliterarystudies\n",
            "\n",
            "3\t0.151\tbut\n",
            "3\t0.150\t. But \n",
            "3\t0.150\t. But\n",
            "3\t0.146\tlove\n",
            "3\t0.142\tof\n",
            "3\t0.098\tone\n",
            "3\t0.093\tdark\n",
            "3\t0.091\tworld\n",
            "3\t0.087\tdiaries\n",
            "3\t0.085\ther , \n",
            "\n"
          ]
        }
      ],
      "source": [
        "gid=0\n",
        "trainingFile = \"%s/train.txt\" % directory\n",
        "devFile = \"%s/dev.txt\" % directory\n",
        "testFile = \"%s/test.txt\" % directory\n",
        "    \n",
        "y_pred, testY, testX, trainY, trainX = run(trainingFile, devFile, testFile)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This result is much better than the performance of the majority class classifier (0.45) and BoW L2 LogReg (0.51).\n"
      ],
      "metadata": {
        "id": "K0f6_wcDcOu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---END OF PART A---"
      ],
      "metadata": {
        "id": "ptAqvs_Ee7Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\pagebreak"
      ],
      "metadata": {
        "id": "pVxMVJ3gfrwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**Analysis Report**"
      ],
      "metadata": {
        "id": "fSvucHmhbX37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Team Members: Mingyuan Ma, Shuyao Zhou, Yuanrui Zhu*"
      ],
      "metadata": {
        "id": "cOmAap5kguR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the model you've trained on your data to tell us something about the phenomenon you've annotated."
      ],
      "metadata": {
        "id": "CprIlRSAbv75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Label Mistaken Identification Using Confusion Matrix & Rethinking Category Boundaries**"
      ],
      "metadata": {
        "id": "ZDLz7ZmTek6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "sns.heatmap(confusion_matrix(testY, y_pred), annot = True, fmt = 'd')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "2zJIpfsZXIF_",
        "outputId": "b2dfdddc-29ed-4493-dbc8-6b8b965840f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'True labels')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class 0\n",
        "class0_precision = 67 / (67 + 27 + 12)\n",
        "class0_recall = 67 / (67 + 10 + 12)\n",
        "class0_f1 = 2 * class0_precision * class0_recall / (class0_precision + class0_recall)\n",
        "\n",
        "print(\"Class 0 precision is %.3f, recall is %.3f f1 score is %.3f\" % (class0_precision, class0_recall, class0_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzrdUttHNSmS",
        "outputId": "4f26e88b-0c42-4bfd-df01-6b9d7e8e9010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 precision is 0.632, recall is 0.753 f1 score is 0.687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class 1\n",
        "class1_precision = 11 / (11 + 10 + 8)\n",
        "class1_recall = 10 / (27 + 10 + 13)\n",
        "class1_f1 = 2 * class1_precision * class1_recall / (class1_precision + class1_recall)\n",
        "\n",
        "print(\"Class 1 precision is %.3f, recall is %.3f f1 score is %.3f\" % (class1_precision, class1_recall, class1_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2s7m0z6Wa3W",
        "outputId": "2b8d8ee4-0096-4164-88ac-7de9b762e12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 1 precision is 0.379, recall is 0.200 f1 score is 0.262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class 2\n",
        "class2_precision = 37 / (37 + 12 + 13 + 3)\n",
        "class2_recall = 37 / (37 + 12 + 8)\n",
        "class2_f1 = 2 * class2_precision * class2_recall / (class2_precision + class2_recall)\n",
        "\n",
        "print(\"Class 2 precision is %.3f, recall is %.3f f1 score is %.3f\" % (class2_precision, class2_recall, class2_f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XIFEIuYWa-7",
        "outputId": "2636c380-7a43-4d54-f048-6338bb87bc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 2 precision is 0.569, recall is 0.649 f1 score is 0.607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plotted the confusion matrix comparing each class of actual and predicted test results, and accordingly calculated the **Precision, Recall and F-1** score for each of the four classes. \n",
        "\t\n",
        "\n",
        "\n",
        "*   For Class 0 (Casual), precision = 0.632, recall = 0.753, F1 = 0.687\n",
        "*   For Class 1 (Ordinary), precision = 0.379, recall = 0.2, F1 = 0.262\n",
        "\n",
        "*   For Class 2 (Formal), precision = 0.569, recall = 0.649, F1 = 0.607\n",
        "*   For Class 3 (Artistic), precision = recall = F1= 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FEloE8prcJN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anlysis on statistics:\n",
        "\n",
        "1. All the classes have the high or low precision and recall at the same time, so clearly there's a gap of performance among all the classes.\n",
        "2. The performance of Casual and Formal is much more better than Ordinary, which follows the general case that Ordinary class is a lot more difficult to classify than the other two classes. However when judging a text to be in class 1, we may involve our personal judgement based on our unique educational and multilingual background. Hence, we're not judging based on the same metrics as machine, and it's normal for machine not capturing these factors and perform badly.\n",
        "3. The score of Casual is the highest among all the classes, which is largely due to the apparent feature it demonstrates to readers (as we stated in guideline).  \n",
        "4. We observed that the logistic model tend to classify real Class 1 to Class 0 or Class 2, since the precision for Class 1 is greater than its recall, which is the opposite case for Class 0 and Class 2. Given for the recall, real Y is in the denominator position, we can infer Class 1 has bearly no apparent features that can be relied on. \n",
        "5. Because of the uneven nature of our dataset, precision and recall all zero for Class 3, which we will conduct further discussion in the follow parts."
      ],
      "metadata": {
        "id": "hwQIerFOm42w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the data set we can clearly conclude that Class 1 (Ordinary) is often mistaken for Class 0 (Casual). Let's see why this would happen by looking at the texts:"
      ],
      "metadata": {
        "id": "RBJeFhhLfHev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 1\n",
        "for i in range(len(testY)):\n",
        "  if testY[i] == '2' and y_pred[i] == '1':\n",
        "    print(str(counter) + \". \" + testX[i])\n",
        "    counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRWRp7x8JapM",
        "outputId": "6bf7760e-b4df-4a58-dee6-0b5af2e8b592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. This is a children’s book and it’s self explanatory rules that you are looking for an exact match. “This” does in fact imply that it is that exact picture you must find in the bigger picture. There are multiple anteaters and you can’t just say any one of them counts because they are all anteaters. I’ve said in other comments as well that I’ve done tons of these books with my kids and that’s how it works. You may find something that looks incredibly similar but if you keep looking you find the exact picture. Every single time. My 5 year old was smart enough to notice it was not the same mongoose but I guess a bunch of Redditors weren’t.\n",
            "\n",
            "2. This was another The Alchemist for me. Everyone in my bookclub loved it and found it deeply insightful. I really wanted to like it but I just found it cliché. I agree with others that the protagonist wasn't relatable... things just kept happening to her. It was a frustrating read.\n",
            "\n",
            "3. This is probably an unpopular opinion but I feel like r/books focuses an ordinate amount of time ranting about things they don't like in physical books rather than discussing what's in them.\n",
            "\n",
            "4. I use a combination of Excel and Goodreads.\n",
            "\n",
            "5. Definitely! Here’s my list….            -only read what you enjoy. If a books not doing it for you put it down. You can always give it another chance later.            -find a quite dim spot with minimal distractions. I always read in bed at night because that’s when everyone else is asleep and won’t interrupt me.          -cut out tv/streaming. Seems weird at first but once you get into a good book tv loses its appeal.            -get rid of social media. Especially FB. It’s just designed to bait you into arguments anyway. I make exceptions for book related platforms like Reddit or Bookstagram. It’s been 3 years and I don’t miss a single thing about FB.-buy cheap used books or use the library. That way you can sift through a wide variety of books for minimal monies. Some used book shops have mint condition books for $4.-join in on the r/52book subreddit. That’s what motivated me to start reading more and it’s been a lot of fun over the years. I found great recommendations and had some really good interactions with other Redditors                -always carry a book with you. You never know when you’ll find an extra 15 minutes to read. And all those little chunks really add up. I did that less in 2021 and you can see the above results.              Those are my main bookish habits that keep me reading at a high rate. Even at 100+ books it doesn’t feel like I’m forcing myself. It’s just reading for pleasure. I hope that helps!\n",
            "\n",
            "6.  Maybe try r/52book? I use Goodreads to set my reading goal for the year and track everything. Other than that, I make time to read every day. Usually at night before bed and on lunch breaks. More on the weekends. I read 81 books this year. I was thinking of trying for 100 but that might be a big stretch and I’m afraid I’d try to read only short books, which would limit the books I want to read that are long. I think maybe having a goal based on number of pages read would be neat too. I read less pages in 2021 than I did in 2020, despite that I read two books more in 2021 than I did the previous year.\n",
            "\n",
            "7. This is why I joined Reddit, /r/booksuggestions, /r/suggestmeabook, /r/52book. After I spent some time learning there, a visit to the library or a real bookstore feels like a trip to Disneyland.\n",
            "\n",
            "8. I actually just finished reading this one as part of my reading challenge this year [link] and I loved it! And yet I completely agree with you. The lesson/s in it are nothing new or overly enlightening by the time you are in your 30s if you have a modicum of self awareness\n",
            "\n",
            "9. Truelit and truefilm are good though.\n",
            "\n",
            "10. Next level reminds me of extreme movie.\n",
            "\n",
            "11. r/badliterature r/askliterarystudies r/literature (the third one is only okay.) for some humor check out r/badreads (bad book reviews) and r/bookscirclejerk (making fun of the (mostly) morons on r/books). i also like r/bookcollecting cause a lot of cool rare books get posted and you can silently judge people’s personal libraries.\n",
            "\n",
            "12. It was a slower week for me. * I read **The Black Tides of Heaven, by Neon Yang**: I loved the worldbuilding behind this silkpunk novella, there’s simply a massive influx of creativity obvious here. That said, I had some issues with the pacing the characterizations, in all it just felt a bit rushed.* 85% complete and will soon finish **Dune Messiah, by Frank Herbert**. The climax is really compelling! I will absolutely be moving ahead with Children of Dune after this. * Done 6 out of 9 short stories in **Exhalation; Stories, by Ted Chiang.** This has been such a wonderful reading experience. Top stories so far are the Merchant and the Alchemist's Gate, Exhalation (the title story), and Truth of Fact, Truth of Feeling (which I read this morning and absolutely adored). * 25% complete **Drive Your Plow over the Bones of the Dead**, **by Olga Tokarczuk.** I'm really enjoying it. Surprisingly laugh-out-loud funny at some points, and just such creative fiction here.\n",
            "\n",
            "13. I know of r/AskLiteraryStudies. Not sure if that's exactly what you want.\n",
            "\n",
            "14. Last year I read 55 books and probably 15-20 I got from this sub or from books posted on r/52book. Thank you everyone.\n",
            "\n",
            "15. Ready Player One. Just listing every 90s pop culture reference you can think of does not a novel make.\n",
            "\n",
            "16. Work on your readers’ advisory (RA)! You can create booklists based on authors, subjects, moods, read alikes (for books and also for tv/movies), or so many other things. For books you’re not actually interested in reading, you can [read a book in 5 minutes][link]\n",
            "\n",
            "17. Every month I try and just pick what I feel like reading \\*within\\* these categories -   1-2 books for my Reading Around the World Challenge1 book for my bookclub1 classic I haven't read1 light and easy romcom type book1 Literary fiction1 from a series I'm reading  After that I just read whatever suits me until the new month.\n",
            "\n",
            "18. Mods may shortly remind you of rule 3\n",
            "\n",
            "19. Charlotte Mason inspired curricula are classical; Ambleside Online is a completely free option with booklists and guides for a secular Charlotte Mason-style curriculum. It’s a lot of work on the parents’ end to gather all the books and set everything up, but it’s a great education model that encourages reading and critical thinking. It’s definitely not just a pile of workbooks to hand the kid to go through, but the prep is worth it. We love it.\n",
            "\n",
            "20. These are tagged as \"general\" /r/BookCollecting/r/bookdiscussion/r/bookent/r/Bookies/r/booklists/r/BookPreviews/r/BookRelatedSubs/r/books/r/BooksAMA/r/BooksForOver30/r/BookshelfDetective/r/booktoursignup\n",
            "\n",
            "21. Recently online-shoplifted an anais nin book, new mascara and face wash from amazon\n",
            "\n",
            "22. In seattle, there’s Ye Olde Curiosity Shoppe and a great horror movie section at MoPop.\n",
            "\n",
            "23. I love multigenerational family sagas, one of my all time favourites is The House of Spirits. ( I’m reading Pachinko atm, completely missed the timing of the r/bookclub even though I was so excited to join in). Read One Hundred few years ago and wanted to re-read with the classics, but Feb was interrupted by holidays and then by sickness so sadly missed all the club reads.\n",
            "\n",
            "24. Sir, round here we don’t approve of “spread betting” this ain’t the bookies.\n",
            "\n",
            "25. Romance is my favorite genre. Authors like Julie Garwood, Lisa Kleypas, Amanda Quick, Julia Quinn, Julie James, Sandra Brown, Elizabeth Lowell, Susan Elizabeth Phillips. I’m sure I’m forgetting a few. But you can get an idea of my preferences.\n",
            "\n",
            "26. I'd say it's definitely low intensity. You can have OSRS open on a second monitor and focus 95% on a movie or tv show while just clicking the green boxes out of the corner of your eye.\n",
            "\n",
            "27. When Jonathan Davis or Marc Thompson read those books to me, I’m just immersed. They could pronounce it cardayayay and I would still believe it's right.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are fewer cases of the opposite, where the prediction is Class 2 when the actual category is Class 1"
      ],
      "metadata": {
        "id": "JG1zN11ueWk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter2 = 1\n",
        "for i in range(len(testY)):\n",
        "  if testY[i] == '1' and y_pred[i] == '2':\n",
        "    print(str(counter2) + \". \" + testX[i])\n",
        "    counter2 += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDoAVMnChwsi",
        "outputId": "3b82a64f-4c73-4879-84e6-66fc9682792f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. It’s really too extensive to list.  I’ll see if I can find some literature on it. There are plenty of options out there, I’m just really surprised at how many people just blindly take all of these supplements (a lot of shitty brands I might add) without knowing whether or not you need it.  Everything is synergistic, so if you over do it on one thing, you can throw off a serious of other metabolic processes.\n",
            "\n",
            "2. I don't think there's a good catch-all term here... though you could check with r/askliterarystudies as it's possible there's something more defined in that world that could be applied here.I've seen people use \"Thematic Shift\" to discuss the technique you seem to be describing. Though, I think that term applies pretty well to *Hereditary* and less so to something like *Mulholland Drive.*\n",
            "\n",
            "3. I'll have to try Story Graph.Zlib has two separate features for keep track of books, Favorites and Booklists. They're both great and really well coded. I much prefer the layout of the booklists to the favorites and I'm trying to migrate all my favorites over there, but you have to do it one book at a time and I have 35 pages of them :(\n",
            "\n",
            "4. AFAIK UK and European PhD applicants typically already have a masters level degree in the field, and you'll be expected to have a thesis project basically ready to go, and you should really have a specific faculty member in mind to work with.\n",
            "\n",
            "5. The bookclub. There's 10 of them I think? You gotta sleep a few times in-between a few to get the next. So have fun making that run 10x lmao. I did em all in a row and it was draining... Idk if it's an unmarked quest you gotta stumble upon or not.\n",
            "\n",
            "6. The one with the book glances to you, then asks if your ok.\n",
            "\n",
            "7. If you look at Jinger's Instagram and scroll way back to May 30th 2017 (about the tenth oldest photo on her Insta) you will find the legendary photo of Jeremy in a library, with the caption, \"Jeremy loves books. I love Jeremy. Now I love books. 😍 📚\"\n",
            "\n",
            "8. If you're unfamiliar with Zen Pencils [link], I strongly recommend reading more about those comics and their background - [link]There is a lot of good material on there.\n",
            "\n",
            "9. Additionally, he beat the shit out of a guy in ring after he found out he was gayTotal lie. Kanyon said in the same book that accuses WWE of having Undertaker hurt him with chairshots that Undertaker was supportive of his struggles.\n",
            "\n",
            "10. Volume, direction, ema, sma. That's, quite literally, all you need if you are able to think fast and react quickly. Day trading is meant to be in and out with 0.5-3%gains (rarely more), and cutting your losses extremely quickly (within 0-0.5%). On average, I'd say I make about 10-20 trades a day. The first 3/4 are in and out because price action isn't there yet but volume is high and there could possibly be a trend so I try and price in accordingly. I'm a market open and near market close trader  because I prefer to play with price action and volume(so 9:30-11am and 2-4pm generally).EMA/SMA for support/resistance levelsVolume, candles, order books, and ticker list to include QQQ, SPY, etc  to gauge general direction. Ultimately, you have to be quick to multitask and assess all this information within seconds to get in early and ride a small trend. Otherwise, it's going to be tough.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking closely at the misclassification cases shows that they're actually closer to Class 1 than we expect them to be. From the perspective of speakers, the person who says 'Next level reminds me of extreme movies’ and ‘Truelit and Truefilm are good though’ may very likely be in a casual state. We may have the wrong tage of those text at the very begining since we didn't quite get what the writer really want to express. \n",
        "\n",
        "Beyond that, we would modify our guideline based on our observations. There're some sentences we would identify as \"plain text\", such as \"I use a combination of Excel and Goodreads.\" We originally defined such “plain text” to be ordinary, but now we would choose to categorize them as Casual rather than Ordinary. We believe that besides \"sentence structure\", \"sentence content\" is another critical measurement of casualness. If the author explains a preference and a quick fact without any specific discussion, this should be categorized as casualness of \"sentence content\", and thus overall casual. \n",
        "\n",
        "Now let's look at other boundaries between Class 2 and Class 3:"
      ],
      "metadata": {
        "id": "NsxMAbcff4E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter3 = 1\n",
        "for i in range(len(testY)):\n",
        "  if testY[i] == '3' and y_pred[i] == '2':\n",
        "    print(str(counter3) + \". \" + testX[i])\n",
        "    counter3 += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWmRVZ5oiV3b",
        "outputId": "8845bb91-b449-479f-8018-364c1c4e4b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. If you read my post you’d see that I had already started one of the books for a book club I joined, so I was definitely not looking for recommendations in that regard. I was looking for comments that would inspire good discussion, like the first comment saying that I should definitely read Ready Player One prior to watching the movie.\n",
            "\n",
            "2. I think like with anything there’s good and bad. The issue is there’s maybe 70-80% bad books out there peddling not just shitty ideas, but sometimes dangerous ones. Anyone who reads these types of books go through a journey where they may discover better and better books. If you want a life hack I’d say the best starting point to void the fluff is to go read some of the classics like Meditations by Marcus Aurelius and a few other stoic classics. You’ll realise that most modern self help is a rehash of these classics. There’s still a few other great ones out there though - I find GoodReads helps weed out a few\n",
            "\n",
            "3. How does r/TrueLit actually think of J. R. R. Tolkien's work? I thought it's the most bizarre thing to discuss here, so I've been hesitant to ask this. Of course, I'll never put Tolkien as the same level as writers as Pynchon, Faulkner or McCarthy in any way. But whenever I read Edmund Wilson's terrible review rampant with ad hominems and even spelling errors for character names, I grimace a lot.\n",
            "\n",
            "4. There is a genetic insensitivity to gut THC adsorption, if I understand the literature correctly.Both my mother and I have this insensitivity and have taken both edibles and sublingual tinctures without any effect even at high doses, while others experienced highness from much smaller doses.\n",
            "\n",
            "5. If this doesn't make sense, please reply with more than a one-word question so I have a better idea of what is confusing you about this.\n",
            "\n",
            "6. Have a search and ask around at r/AskLiteraryStudies.\n",
            "\n",
            "7. Audra Simpson's *Mohawk Interruptus*You might also want to ask over at /r/AskLiteraryStudies too.\n",
            "\n",
            "8. I should have said at the outset that I will conduct this discussion like a university seminar.  Questions of belief or appeals to authority or dogma will not enter in.  The basis will be historical evidence and analysis and academic literature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter4 = 1\n",
        "for i in range(len(testY)):\n",
        "  if testY[i] == '2' and y_pred[i] == '3':\n",
        "    print(str(counter4) + \". \" + testX[i])\n",
        "    counter4 += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqfHKL9Li-eI",
        "outputId": "c50bf7f4-9851-4ad6-d05f-c8d41875538e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. To be fair, the only poets he thinks are good are [\"Homer, Virgil, Dante, Chaucer, Shakespeare, Milton, Goethe and maybe Dickinson\"][link], so it's a pretty high bar (especially since you assume he must have read them in their original languages to make such a pronouncement). [He's also 19][link], so there is still a little hope that he'll grow out of it.\n",
            "\n",
            "2. There are cutscenes that came out today in game that are covered in the novel. There are no other spoilers and nothing for future content, it appears.\n",
            "\n",
            "3. I'm not sure if such a website or program exists and - as a literary scholar by training - I am somewhat skeptical that such an approach would do a good job at teaching the necessary skills to critically engage with literature. I could very much see it run the risk of  boiling literary analysis down to nothing but a checklist of actionable steps and removing any kind of ambiguities (and thus potential) from literary texts.While there are certain protocols to follow in literary studies, the craft is much more free form than, say, running a cost-benefit analysis. Instead, it is a matter of being receptive to the nuances of a text - and to bring one's understanding of (literary) theory and history into dialogue with these nuances. (I am, of course, oversimplifying.)Thus, building a personal repertoire of literary theories and a solid understanding of (literary) histories might be one of the best bets to become better at critically engaging with literature. Building this repertoire, in turn, is probably best achieved through reading - and discussing what one has read.When it comes to the former, you can find a few recommendations in the following:If you want to dip your toes into some pop-literary studies first, you might want to try something like Thomas C. Foster's *How to Read Literature Like A Prof*, which u/pm_me_poemsplease recommended in a similar context.Another great way to get a first feel for almost any academic field are Oxford University Press's Very Short Introductions - in this case Jonathan Culler's Literary Theory: A Very Short Introduction.You could also have a look at general introductory textbooks, like Mario Klarer's *An Introduction to Literary Studies,* or at textbooks focusing specifically on literary theory, such as erry Eagleton's *Literary Theory: An Introduction* and Peter Barry's *Beginning Theory: An Introduction to Literary and Cultural Theory*. In case you would prefer to watch a lecture on the topic, there is always [Yale's Open Course on Literary Theory](https://oyc.yale.edu/english/engl-300).Finally, if you want to dive straight into the deep end, you could have a look at Cambridge University Press's extensive list of *Cambridge Companions to X* and *Cambridge Histories of Y*, where X and Y are something like \"the Victorian novel\" and \"American Literature.\"I hope some of this helps and I am sorry I couldn't offer any more open-source resources.If you've got any questions, feel free to ask!EDIT: You might also want to check out r/AskLiteraryStudies. :)\n",
            "\n",
            "4. The subreddit is titled ‘AskLiteraryStudies’.That’s what the OP did. If you want, start a subreddit called ‘Don’tAskResearchQuestionsYouCouldGoogle’. Your indignation when confronted with a similar polite request will then be appropriate.\n",
            "\n",
            "5. Our kids prefer to own most of their books so they can share and re-read. Libraries have book sales. Try booksalefinder.com. The Mensa website has good booklists broken down by age group. You can also google top 100 kids chapter books of all time and start there.\n",
            "\n",
            "6. This is a sub for pictures of people's bookshelves. /r/books, /r/bookdiscussion, /r/paranormalromance, and /r/YALit would be better places for this.\n",
            "\n",
            "7. Just for fun, I gave a relevant prompt to an [outside link] to see what it would come up with: **(Prompt) An open letter by Franz Kafka about the dearth of novelty in /r/TrueLit:** I have just read your statement on the subject of novelty. You are correct, I do not know how to express the thing, because I myself do not know how to express the thing. Yet the thing is the thing. The thing is what the thing is. The thing is what the thing is. I will not insult your intelligence by saying that the thing is the thing. You know it is the thing. I know it is the thing. It is the thing. The thing. The thing. The thing. The thing. The thing. The thing. The thing. The thing. The thing. I tried the same prompt again, just with a different writer. **(Prompt) An open letter by Fyodor Dostoevsky about the dearth of novelty in /r/TrueLit:** We have been reading and rereading the article in which you have described the monotonous state of your life and the “dearth of novelty” in it. It has a “sad and gloomy” look. Your heart aches as you think of it. But the article is not gloomy, not sad. It is, rather, quite the opposite. The last one I tried was strangely ... sensible. Not what I was expecting.\n",
            "\n",
            "8. Who's we? He obviously wanted to, and since it's his novel, I don't see why not. Also, this is truelit, I come here for in-depth literature discussion, not to be reminded of reddits tendency to review books they don't plan on reading.\n",
            "\n",
            "9. Check out the weekly suggestions thread(should be stickied), the mod recommendation in the sidebar, /r/booksuggestions , or  /r/whattoreadwhen\n",
            "\n",
            "10. This is my favorite book list: [link]\n",
            "\n",
            "11. I know that you're specifically asking for the opinion of a _historian_, but your question lies at the intersection of literature and history. An academic trained in language and literature might find some parts of your question easier to weigh in on with authority. You might benefit from posing this to r/AskLiteraryStudies, too.\n",
            "\n",
            "12. Written in rich, seductive prose, this novel contains senses and the experience.\n",
            "\n",
            "13. There's also the [weekly recommendation thread](http://www.reddit.com/r/books/comments/2mgd3p/weekly_recommendation_thread_for_the_week_of/) and the various recommendation subs pulled from this [List of book related subs](http://www.reddit.com/r/BookRelatedSubs/wiki/sublist)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the boundary between Class2 (ordinary) and Class3 (formal) is more vague than the boundary between casual and ordinary since it is hard or there is no signs/features of formal that can make us believe that one document is formal. Unlike Class1 1 we have some clear signs, we need to consider a document holistically and apply our prior exerience which is subjective and different among people. \n",
        "\n",
        "We can still find some cases of misclassification. The first observation we have from misclassification is that serval words that happen in infomal document with high frequency, may lead a formal document to be classified as informal. As what I mention above, we need to view the document holistically, including words, tone, emotion, etc. rather than basing on a particualr words. However, we can see that the machine may classify one document based on some typical words. It is not wrong but it may encounter some edge cases, where the majority of one document is not informal in machine sense, but one word is informal, then the document would be classify as informal. It's crutial for viewing the document holistically. For example, given the sentence \"Audra Simpson's Mhawk Interrupts you might also want to ask over at r/AskLiteraryStudies too,\" it should be classify as 3 since the author of this post is giving advices from content perspective, but in a more more likely senario, the fact that the word \"too\" present in the document leads the document to be classified as 2. \n",
        "\n",
        "The second observation from misclassification makes is that we feel thhe model ignores the dependency or correlation between one words (token) with the whole document; since we use unigram, bigram, trigram (word combinations) as features, we cannot capture one combination's infomration perfectly. Since a words-pair (may be 1, 2, or 3) may still have correlation or dependency with the whole document. However, we are using finite gram, we may not be able to capture it or in some cases, may overfit the model. Hence, we can notice that \"70-80%\" may be regarded as sign to be Class 2 because other document may use 70-80% in a different way. However, 70-80% is statistics which bolster main idea with evidence, which should be a optimal way to make the document more formal. The machine cannot capture the weightage of each feature (which is hard even for us human since language is subjective, and no dounbt n-gram does worse). For example, \"just for fun\", \"To be fair\", with other obvious formal/professional words would lead it to be Class 2, but the machine classify them as class-3. \n",
        "\n",
        "The third observation from misclassification is how to deal withh those abbreviation (mis-spell) words in the post and the position of the words. Since we are analyzing the posts from reddit, and some documents may contain abbreviation of words, which is a feature of casual; however, those topics are ranging more different book, product, daily langauge use, so it is hard to include all the potention abbreviations, espcially some of the abbreviated words are not common, which would possibly be a particular topic or book. More importantly, it's hard for us to capture mis-spelling words. since there are uncountable ways to miss spell a word. Unfortunately, it is important to capture them as features. \n",
        "\n",
        "However, the observations from misclassification also help us a lot in clarifying the boundary. For example, some words contain emotions may be captured by the model, which also make us realize that we can clarify the boundary through different dimensions toward each document. Not only words themselves, content, but also the tone and degree of emotion, which we could clarify more for making the boundary as clear as possible."
      ],
      "metadata": {
        "id": "9Ev22Ws2rUWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Level of Balance**"
      ],
      "metadata": {
        "id": "RoiMrBL5kP-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Percent(float):\n",
        "    def __str__(self):\n",
        "        return '{:.2%}'.format(self)"
      ],
      "metadata": {
        "id": "eBJLGHp8mnb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Class 0 percentage: ', Percent(trainY.count('1') / len(trainY)))\n",
        "print('Class 1 percentage: ', Percent(trainY.count('2') / len(trainY)))\n",
        "print('Class 2 percentage: ', Percent(trainY.count('3') / len(trainY)))\n",
        "print('Class 3 percentage: ', Percent(trainY.count('4') / len(trainY)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nlrgrNMlYzb",
        "outputId": "5566a015-359b-4304-cd0b-cea492a20382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 percentage:  40.67%\n",
            "Class 1 percentage:  26.83%\n",
            "Class 2 percentage:  31.33%\n",
            "Class 3 percentage:  1.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One flaw of the training set and testing set is the imbalance of our data.\n",
        "\n"
      ],
      "metadata": {
        "id": "1jz6XsaSjYqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class 3 (Artistic) is not quite prevalent, which puts some limitations on the ability of training, pushing the algorithm to ignore the underrepresented Class entirely. This explains among the three artistic labels in the test set, none of them are being classified correctly. Much more training set should be involved in order to capture the internal patterns and rhetorical devices of the artistic paragraphs."
      ],
      "metadata": {
        "id": "Yz_SIrhcomUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, one way to deal with this problem is **randomly oversampling Class 3**. "
      ],
      "metadata": {
        "id": "p2iAdbY8kZ3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MddqPbUtojra",
        "outputId": "ec12568e-0cf7-44fc-9e3a-16710dbe1150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "metadata": {
        "id": "SLmXu5nQoy0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make another classifier to perform oversampling\n",
        "class Classifier_2:\n",
        "\n",
        "    def __init__(self, feature_method, trainX, trainY, devX, devY, testX, testY):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.min_feature_count=2\n",
        "        self.log_reg = None\n",
        "        \n",
        "        oversample = RandomOverSampler(sampling_strategy=\"minority\")\n",
        "\n",
        "       \n",
        "        self.devY=devY\n",
        "        self.testY=testY\n",
        "        \n",
        "        tX_2d = self.process(trainX, training = True)\n",
        "\n",
        "        tX_2d = tX_2d.toarray()\n",
        "        \n",
        "        tX_2d, self.trainY = oversample.fit_resample(tX_2d, trainY)\n",
        "\n",
        "        self.trainX = sparse.csr_matrix(tX_2d)\n",
        "\n",
        "        print(\"Training set has \", self.trainX.shape, \" data now.\")\n",
        "\n",
        "        self.devX = self.process(devX, training=False)\n",
        "        self.testX = self.process(testX, training=False)\n",
        "\n",
        "        self.predict = []\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append(feats)\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, X_data, training = False):\n",
        "        \n",
        "        data = self.featurize(X_data)\n",
        "        \n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        \n",
        "        for idx, feats in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "        \n",
        "        return X\n",
        "      \n",
        "    def train(self):\n",
        "      (D,F) = self.trainX.shape\n",
        "      best_dev_accuracy=0\n",
        "      best_model=None\n",
        "      for C in [0.1, 1, 10, 100]:\n",
        "          self.log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n",
        "          self.log_reg.fit(self.trainX, self.trainY)\n",
        "          training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "          development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "          if development_accuracy > best_dev_accuracy:\n",
        "              best_dev_accuracy=development_accuracy\n",
        "              best_model=self.log_reg\n",
        "\n",
        "             #print(\"C: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (C, training_accuracy, development_accuracy))\n",
        "\n",
        "      self.log_reg=best_model\n",
        "\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "        predict = self.log_reg.predict(self.testX)\n",
        "        self.predict = predict\n",
        "        \n",
        "        return self.log_reg.score(self.testX, self.testY)\n",
        "        \n",
        "\n",
        "\n",
        "    def get_predict(self):\n",
        "      # print(self.predict)\n",
        "      return self.predict"
      ],
      "metadata": {
        "id": "F93pqO2fvejB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_2(trainingFile, devFile, testFile):\n",
        "    trainX, trainY=load_data(trainingFile)\n",
        "    devX, devY=load_data(devFile)\n",
        "    testX, testY=load_data(testFile)\n",
        "\n",
        "    simple_classifier = Classifier_2(weighted_convoluation_featurize, trainX, trainY, devX, devY, testX, testY)\n",
        "    simple_classifier.train()\n",
        "    accuracy=simple_classifier.test()\n",
        "\n",
        "    y_pred = simple_classifier.get_predict() \n",
        "\n",
        "    lower, upper=confidence_intervals(accuracy, len(testY), .95)\n",
        "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
        "\n",
        "    return simple_classifier.trainY, y_pred, testY\n",
        "    "
      ],
      "metadata": {
        "id": "WaTAnFAEp39k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY2, y_pred2, testY2 = run_2(trainingFile, devFile, testFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FJfvJIpqjvB",
        "outputId": "c209634b-12f5-46bd-8bc4-013dc4dd50f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has  (837, 13048)  data now.\n",
            "Test accuracy for best dev model: 0.570, 95% CIs: [0.501 0.639]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainY2)"
      ],
      "metadata": {
        "id": "llwHeIpd10_3",
        "outputId": "2fd2c328-b3f3-4bf3-eae3-b8581b3f38ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "837"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(confusion_matrix(testY2, y_pred2), annot = True, fmt = 'd')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "CBSjI3rB7cTy",
        "outputId": "ab74e02f-0847-4e6b-ac30-b6e593832017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'True labels')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEGCAYAAABIGw//AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU9ZnH8c/Tw3AKKKI4HAYTiGJE0HAJK0E5PVB2k0BMMGo0hAQVj8R7NZqYuDHBaKLJsl54oXhfiLIIi0blFJVLOYWZ4VBk5IaZnmf/6BoYdGa6h+nuqhm+b1/1mq6q7l891Kt95jdP/epX5u6IiEj0xMIOQEREKqYELSISUUrQIiIRpQQtIhJRStAiIhFVL+wAKlP8+UoNLwn07XJx2CFExuE5jcMOITKmrF8QdgiRUbKnwGraRnVyTm7Lb9b4eKlQD1pEJKIi24MWEcmq0njYEXyNErSICEC8JOwIvkYJWkQEcC8NO4SvUYIWEQEoVYIWEYkm9aBFRCJKFwlFRCJKPWgRkWhyjeIQEYkoXSQUEYkolThERCJKFwlFRCJKPWgRkYjSRUIRkYjSRUIRkWhyj14NWvNBi4hAogad6pKEmR1qZs+Y2VIzW2Jmp5hZCzObambLgp+HJWtHCVpEBBIljlSX5O4Gprj7cUAXYAlwHTDN3TsC04L1KilBi4hA2nrQZtYc6As8AODue9y9CDgXmBC8bQIwLFlIStAiIgDx4pQXMxtlZnPLLaPKtXQM8BnwkJm9b2b3m1kToJW7rwvesx5olSwkXSQUEYFqjeJw9/HA+Ep21wNOBi5z91lmdjdfKWe4u5tZ0ofUqgctIgLpvEiYD+S7+6xg/RkSCXuDmeUBBD83JmtIPehytmzdxi13/JXlKz8FM353w5V0PaETjz/9Ik8+9wqxWIy+vXtw9ZiLww41o278yzX0HtCLzZ8XMbL/zwBodmhTfvePm8lrdxTr1q7nptG3svXLbSFHmnlj7xxL9/49+HJTEWMGjgFg5NUj6TmoF17qFG0q4q9X38UXG74IOdLsGzyoH+PG3UZOLMaDD03kT3feG3ZINZOmcdDuvt7M1prZse7+MdAfWBwsFwB3BD9fTNaWuSftZYei+POVWQ/sht/9mZO7nMAPzhlCcXExO3ftZuknKxj/yJPcd+et1K9fn02bizj8sEOzGlffLtn9hdC154ns2L6Tm+++fm+CHnPjL9hStIVH753I+WPOo2nzptz3h8r+wsucw3MaZ/V43+nxHXbt2MVVd121N0E3OqQRO7ftBGDoRUM5uuPR3HtD9pPTlPULsn7MMrFYjCWL3mLImeeRn7+O996dzMjzf8WSJctCiadkT4HVtI1dbz2acs5peOr5VR7PzLoC9wP1gZXARSQqFpOAo4FPgeHuXuVvdpU4Alu3bWfeBwv5/tDBAOTm5tKs6SE89cKrXDxyOPXr1wfIenIOw4JZH7KlaMt+204d3JvJT78OwOSnX6fvkD5hhJZ1i2YvYmvR1v22lSVngIaNGxLVTk4m9eh+EitWrGbVqjUUFxczadKLnBP8v1Nbebw45SVpW+4L3L2bu5/o7sPcfbO7b3L3/u7e0d0HJEvOoBLHXgWF6zns0ObcdPs4Pl6+kuOP7ch1V4xm9ZoC5n2wkHvGT6BB/VyuvvQSOnc6Nuxws65FyxZs2pj4Pm3a+AUtWrYIOaJwnf+bn3L6909nx9btXD/i+rDDybrWbY5ibX7h3vX8gnX06H5SiBGlQQQnS8pYD9rMjjOza83snmC51sw6Zep4NVUSj7Pkk+WM+PezeObhe2nUqCEPPDqJeDzOli1beWL8XVw95hJ+/Z9/PCh7TF91sJ+DR+98hIt6XciMF2Zw9oVDww5H0iG9N6qkRUYStJldCzwJGDA7WAyYaGaV3j1Tfmzh/Y9MzERolTrqyJa0OqIlJ37nOAAG9fs3Fn+ynFZHtmTA9/pgZnQ+/ljMjM1FX2Y1tij44vMvOPzIRK/58CNbsHnT5pAjioYZz8+gzxm9ww4j6woL1tOubeu9623b5FFYuD7EiNIgjbd6p0umetAXA93d/Q53fyxY7gB6BPsq5O7jg7pNt0t+el6GQqtYy8NbcNSRR7Dq03wA3pu3gG+1P5rTTz2F2fM/AGD1mnyKS0o47NDmWY0tCt5+4x3O/GGixnjmDwfz1uvvhBxReFq335eYeg7qRf6K/BCjCcecuQvo0OEY2rdvR25uLsOHn8vLr7wRdlg1E8EedKZq0KVAaxJXKsvLC/ZF0g1X/pJrb/0TxSXFtGudx+9uuJLGjRpy0x/uYtjI0eTm1uMPN12NWY0vGEfarffexMmndOXQFs15ce4k7v/zwzxy70Ru/+ctDD3vTNbnb+Cm0beGHWZW/OZv19D5lM40O6wZD8+awOPjHqfbad1o+602lJY6nxVs5N7ra/nwsgMQj8cZe8VNTH71CXJiMR6e8BSLF38Sdlg1E8EadEaG2ZnZEODvwDJgbbD5aKADcKm7T0nWRhjD7KIq28Psoizbw+yiLMxhdlGTjmF2O1/9a8o5p9FZV2Sll5aRHrS7TzGzb5MoabQJNhcAczyKk66KiESwB52xYXbuXgq8l6n2RUTSSk9UERGJqIOpBy0iUquoBy0iElHqQYuIRFRJSdgRfI0StIgIQASnL1CCFhEB1aBFRCJLCVpEJKJ0kVBEJKLi0bvJWQlaRARU4hARiSwlaBGRiFINWkQkmrxU46BFRKJJJQ4RkYjSKA4RkYhSD1pEJKLSmKDNbDWwFYgDJe7ezcxaAE8B7YHVwHB331xVO5l6qreISO3invqSmtPcvau7dwvWrwOmuXtHYFqwXiUlaBERSPSgU10OzLnAhOD1BGBYsg8oQYuIAJR6youZjTKzueWWUV9pzYE3zGxeuX2t3H1d8Ho90CpZSJGtQU868eawQ4iMBrGcsEMQqfuqMYrD3ccD46t4y7+5e4GZHQlMNbOlX/m8m1nSWklkE7SISDZ5Gi8SuntB8HOjmT0P9AA2mFmeu68zszxgY7J2VOIQEYFqlTiqYmZNzKxp2WtgELAQeAm4IHjbBcCLyUJSD1pEBNI5F0cr4Hkzg0SOfcLdp5jZHGCSmV0MfAoMT9aQErSICCTtGafK3VcCXSrYvgnoX522lKBFRABKdKu3iEg0abpREZGI0nSjIiLRlM5hdumiBC0iAupBi4hElhK0iEhEacJ+EZFo0jMJRUSiSglaRCSiNIpDRCSi1IMWEYkoJWgRkWjyuEocIiLRpB60iEg0aZidiEhUKUGLiERU9ErQStAiIgBeEr0MrQQtIgLqQUdZ49YtOOXu0TQ6ojnuzvLHpvPxA6/zb/+8lKbfygOgfrPG7Nmyg9cG3hhytJl1zZ9/zSkDelL0eREXDfg5AN87qy8XXvVTvtHxaH559qV8/OEnIUeZHWPvHEv3/j34clMRYwaOAWDk1SPpOagXXuoUbSrir1ffxRcbvgg50uwbPKgf48bdRk4sxoMPTeRPd94bdkg1EsWLhLGwA4iK0pJS5t/2BK/0u5bXz/4t375wAM06tubt0X/ntYE38trAG1n76hzWTp4TdqgZN+Xp17lm5PX7bVv18Wpu/vlv+XDWRyFFFY7/ffp/ueWnN++37dn/fpbLBl/K5Wdcxpxpszlv7HkhRReeWCzGPXffztlDR9K5y2mMGDGMTp06hh1WzZRWY8kSJejAro1FbP5oNQAl23fx5fJCGue12O89R5/Tk09feDeE6LLrw1kfsbVo637b1ixfw9qV+SFFFJ5Fsxd97Vzs3LZz7+uGjRviHr2eV6b16H4SK1asZtWqNRQXFzNp0oucM3Rw2GHViJd6yku2qMRRgSZtW9LihG/w+fwVe7cd2fNYdn32JVtXbQgxMomK83/zU07//uns2Lqd60dcn/wDdUzrNkexNr9w73p+wTp6dD8pxIjSIII16Kz3oM3soir2jTKzuWY2980dy7IZ1l71Gjfg1PvHMu/mxygp11P6xrBTWH0Q9J4lNY/e+QgX9bqQGS/M4OwLh4YdjqSBl6S+ZEsYJY5bK9vh7uPdvZu7dzu9cfbrWVYvh1PvH8vq595h7Wtz923PidHuzO58+tKsrMck0Tbj+Rn0OaN32GFkXWHBetq1bb13vW2bPAoL14cYUc15aepLKswsx8zeN7NXgvVjzGyWmS03s6fMrH6yNqqVoM3sMDM7MYX3fVjJ8hHQqjrHzKZef7mELcsKWTr+tf22H3XqCWxZXsjOdQfflXr5utbt9yWmnoN6kb/i4KvNz5m7gA4djqF9+3bk5uYyfPi5vPzKG2GHVTPpv0g4FlhSbv2/gLvcvQOwGbg4WQNJa9BmNgM4J3jvPGCjmf3L3a+q4mOtgMFBEPs1B7yT7JhhOKLHt/nmD09l8+I1nDH1dgA++OMkCt/8gG+c2+uguDhY5j//fgNdT+lC8xbNeXrORB76ywS2FG1l7O8upXmL5vxxwu0sX7SCa0ZeF3aoGfebv11D51M60+ywZjw8awKPj3ucbqd1o+232lBa6nxWsJF7r6/dw8sORDweZ+wVNzH51SfIicV4eMJTLF5cu4deptozToWZtQXOAm4HrjIzA04Hfhy8ZQLwW+AfVbaT7Aq0mb3v7ieZ2SVAO3e/xcw+dPdKe9Jm9gDwkLu/XcG+J9z9xxV8bD+Ptx558F0ar8T/xGr3n47pdEisQdghRMaU9QvCDiEySvYUWE3b2Nj/eynnnFZvzvwFMKrcpvHuPr5sxcyeAf4INAV+DVwIvBf0njGzdsBr7n5CVcdJZRRHPTPLA4YDKd2h4e6Vdt1TSc4iItnm8dRzfJCMx1e0z8zOBja6+zwz61eTmFJJ0LcBrwNvu/scM/smEM4QCxGRDEljiaMPcI6ZnQk0BJoBdwOHmlk9dy8B2gIFyRpKepHQ3Z929xPd/VfB+kp3/36NwhcRiRgvtZSXKttxv97d27p7e+BHwJvu/hNgOvCD4G0XAC8mi6nSHrSZ/Q2otCbj7pcna1xEpLZI50XCSlwLPGlmvwfeBx5I9oGqShxzq9gnIlKnuNf4OmMFbfoMYEbweiXQozqfrzRBu/uE8utm1tjdd1Q/RBGR6MtCD7raktagzewUM1sMLA3Wu5jZfRmPTEQki0rjlvKSLancSfhXEjedbAJw9w+AvpkMSkQk29J1kTCdUprNzt3XJm6E2SuemXBERMKRzcSbqlQS9Foz6w24meXy9fvLRURqvShO651Kgh5NYpB1G6CQxE0rYzIZlIhIttXKHrS7fw78JAuxiIiEJhPD7GoqlVEc3zSzl83sMzPbaGYvBrd7i4jUGfG4pbxkSyqjOJ4AJgF5QGvgaWBiJoMSEck2d0t5yZZUEnRjd3/U3UuC5TESE4CIiNQZtWqYnZmVPdL6NTO7DniSxNwcI4DJWYhNRCRratsojnkkEnLZr4tflNvnwMH3KGMRqbNq1SgOdz8mm4GIiIQpXhrGM7SrltKdhGZ2AnA85WrP7v5IpoISEcm22lbiAMDMbgH6kUjQk4EzgLcBJWgRqTNKa+M4aBJPAOgPrHf3i4AuQPOMRiUikmVRHGaXSoljp7uXmlmJmTUDNgLtMhyXiEhW1coSBzDXzA4F/ofEyI5twLsZjQqYkPN5pg9Ra2wu1nMSyjzUKnp/hoal4/qwI6hboljiSGUujl8FL/9pZlOAZu7+YWbDEhHJrlo1isPMTq5qn7vPz0xIIiLZF8EKR5U96L9Usc+B09Mci4hIaGpVicPdT8tmICIiYYridKMp3agiIlLXRfCh3krQIiIATvR60NG7bCkiEoISt5SXqphZQzObbWYfmNkiM7s12H6Mmc0ys+Vm9pSZ1U8WUypPVDEzG2lmNwfrR5tZjxT/zSIitYJjKS9J7AZOd/cuQFdgiJn1Av4LuMvdOwCbgYuTNZRKD/o+4BTgvGB9K3BvCp8TEak1SquxVMUTtgWrucFSNvLtmWD7BGBYsphSSdA93X0MsCs4+GYgaddcRKQ2qU4P2sxGmdnccsuo8m2ZWY6ZLSAxNcZUYAVQ5O4lwVvygTbJYkrlImGxmeUQjOM2syOI5gVPEZEDVp2k5u7jgfFV7I8DXYNpMp4HjjuQmFJJ0PcEBzjSzG4nMbvdTQdyMBGRqIpnYBSHuxeZ2XQSZeJDzaxe0ItuCxQk+3wqc3E8bmbzSEw5asAwd19Sw7hFRCIlXU+8CqoMxUFybgQMJHGBcDqJDu6TwAXAi8naSmXC/qOBHcDL5be5+5oDC19EJHpK09eDzgMmBKXhGDDJ3V8xs8XAk2b2e+B94IFkDaVS4niVfQ+PbQgcA3wMfOcAgxcRiZx0TZYUzPZ5UgXbVwLVGqKcSomjc/n1YJa7X1XydhGRWimKIx+qfau3u883s56ZCEZEJCylFr1bvVOpQV9VbjUGnAwUZiwiEZEQxMMOoAKp9KCblntdQqIm/WxmwhERCUe6RnGkU5UJOrgK2dTdf52leEREQpHGURxpU9Ujr+q5e4mZ9clmQCIiYahtj7yaTaLevMDMXgKeBraX7XT35zIcm4hI1tS6EkegIbCJxExMZeOhHahTCfqqP19Jr/49KdpUxKgBowH4+Y2X0GtAT4qLS1j3aSF/vnoc27dsT9JS3TNy1Aj+4ydDcYdlS1Zw8xW3s2f3nrDDygqrn0veQ+MgNxerl8P2qW9R9I9HyHtoHNa4MQA5LQ5l98KlbLzyt+EGm2WDB/Vj3LjbyInFePChifzpzto9yWUUh9lVNZvdkcEIjoXAR8HPRcHPhVmILaumPj2VG87ff4qR+W/N5+cDfsHoQb8kf2UBPxozIqTownPkUS358SU/5LzBP+P7/UYSy4kxZNiAsMPKGt9TzLpLfkPh8NEUDB9Noz7daNC5E+suuorCEaMpHDGa3R8uZse0t8MONatisRj33H07Zw8dSecupzFixDA6deoYdlg1ErfUl2ypKkHnAIcES9Nyr8uWOuWjWQvZWrR1v23zZs6nNJ74vbr0/aUckdcyjNBCl5OTQ4OGDcjJyaFRo4Z8tv7zsEPKKt+5CwCrVw+rV4/y1Upr0piGPbqyffo7IUUXjh7dT2LFitWsWrWG4uJiJk16kXOGDg47rBpJ13zQ6VRViWOdu992oA2b2XEk5judVW7yasxsiLtPOdB2wzJ4+CD+7+WZYYeRdRvXf86Ef0zk9XnPs2vXbt6dMZt3/2922GFlVyxG64n3kXt0a7Y89RK7P1q6d1eT03qza9b7+PYdIQaYfa3bHMXa/H23Q+QXrKNH96/d3Vyr1LYSxwF35M3schIzNV0GLDSzc8vt/kMVn9s7CXb+trUHevi0O++yHxGPx5n2/Jthh5J1TZs35bQhp3Jmjx8wsMs5NGrciLO+X7t7StVWWkrhiNGsHXQeDU44ltwO7ffuanLGaWx7bXp4sUnauKW+ZEtVCbp/Ddr9OfBddx8G9AP+08zGBvsq/ee5+3h37+bu3doe0q4Gh0+fgT8cSM/+Pbnjsj+FHUooevXtRsGaQjZvKqKkJM60yTPo0r1z8g/WQaVbt7Nrzgc06t0NgNihzWhwwnHsfGtWyJFlX2HBetq1bb13vW2bPAoL14cYUc1FscRRaYJ29y9q0m5ZWcPdV5NI0meY2Thq0DPPtm79vsvw0T/glp/9lt27docdTijW52/gxO9+h4aNGgDQ89RurFq2Otygsih2WHNiTZsAYA3q06jXyRSvTvx112RgX3bMfA/fUxxmiKGYM3cBHTocQ/v27cjNzWX48HN5+ZU3wg6rRuLVWLKl2pMlpWiDmXV19wUA7r7NzM4GHgQi2f26/u/XcWKvE2neohmPz36UR//yGCMuHUH9+rnc8USiKrNk/lLuueFvIUeaXR+9v5ipr0znyTceJh6Ps/SjT3jm0aTzjNcZOS1bcMTvr8FiMYgZ29+Yyc6ZiR5zk8H9+PLBJ0OOMBzxeJyxV9zE5FefICcW4+EJT7F48Sdhh1UjURwHbe7pv3/GzNoCJe7+tb95zKyPu/8rWRuD2g2J4o09odhQvCXsECLjhVZNwg4hMjouWRx2CJFRsqegxun1rqNHppxzrlzzWFbSeUZ60O6eX8W+pMlZRCTbojiKI1MlDhGRWiWKf7IrQYuIEM0atBK0iAi1d8J+EZE6rzSCRQ4laBERdJFQRCSyotd/VoIWEQGi2YOuai4OEZGDRol5yktVzKydmU03s8VmtqhsHiIza2FmU81sWfDzsGQxKUGLiJAocaS6JFECXO3uxwO9gDFmdjxwHTDN3TsC04L1KilBi4iQvtns3H2du88PXm8FlpCYG/9cYELwtgnAsGQxKUGLiJAYZpfqUn7u+mAZVVGbZtYeOAmYBbRy93XBrvVAq2Qx6SKhiAjVG8Xh7uOB8VW9x8wOAZ4FrnD3LWb7blV0dzdLUsxGPWgRESC9E/abWS6J5Py4uz8XbN5gZnnB/jxgY7J2lKBFRIA4nvJSFUt0lR8Alrj7uHK7XgIuCF5fQOKxgFVSiUNEhLSOg+4DnA98ZGYLgm03AHcAk8zsYuBTYHiyhpSgRUQAT9O9hO7+NpU/2q9az3pVghYRIZp3EipBi4ig2exERCIreulZCVpEBICSCKZoJWgREdJ3kTCdIpug39zwUdghSAR9d3vjsEOQOkoXCUVEIko9aBGRiFIPWkQkouKuHrSISCRpHLSISESpBi0iElGqQYuIRJRKHCIiEaUSh4hIRGkUh4hIRKnEISISUbpIKCISUapBi4hElEocIiIR5bpIKCISTXH1oEVEokklDhGRiFKJQ0QkoqLYg46FHYCISBR4Nf5LxsweNLONZraw3LYWZjbVzJYFPw9L1o4StIgIiVu9U11S8DAw5CvbrgOmuXtHYFqwXiUlaBEREiWOVJdk3H0m8MVXNp8LTAheTwCGJWtHCVpEhOolaDMbZWZzyy2jUjhEK3dfF7xeD7RK9gEl6EoMHtSPRQtnsnTx21zzmzFhhxMqnYuEBg3qM3X6M8x85yXemT2Z6264POyQQlXXvhfuXp1lvLt3K7eMr+axHJJ3xZWgKxCLxbjn7ts5e+hIOnc5jREjhtGpU8ewwwqFzsU+u3fvYdjZP6Vv73Po2/sc+g/oS7fuXcMOKxR18XuRzhJHJTaYWR5A8HNjsg8oQVegR/eTWLFiNatWraG4uJhJk17knKGDww4rFDoX+9u+fQcAubn1qJdbL5JjZ7OhLn4v0jmKoxIvARcEry8AXkz2ASXoCrRucxRr8wv3rucXrKN166NCjCg8Ohf7i8Vi/N+/XuLjle8xY/q/mDf3g7BDCkVd/F7EvTTlJRkzmwi8CxxrZvlmdjFwBzDQzJYBA4L1KmXsRhUz60Gi1DLHzI4nMeRkqbtPztQxRTKttLSU7/U5h2bNm/LoE/fRqVNHlixZFnZYkgbp/GvI3c+rZFf/6rSTkQRtZrcAZwD1zGwq0BOYDlxnZie5++2VfG4UMArAcpoTizXJRHhJFRasp13b1nvX27bJo7BwfSixhE3nomJbvtzK2zNn0X9g34MyQdfF78XBdCfhD4A+QF9gDDDM3X8HDAZGVPah8ldGw0rOAHPmLqBDh2No374dubm5DB9+Li+/8kZo8YRJ52Kfw1u2oFnzpgA0bNiAfqf35pNPVoYcVTjq4vciCzXoastUiaPE3ePADjNb4e5bANx9p5lF8cky+4nH44y94iYmv/oEObEYD094isWLPwk7rFDoXOzTqtUR3PfffyInJ0YsFuOF517jjSnTww4rFHXxe1EawQu+lomr0GY2CzjN3XeYWcw9UVU3s+bAdHc/OVkb9eq3id7ZktA1a9A47BAiY8vuHWGHEBklewqspm18p1XPlHPOog2zany8VGSqB93X3XcDlCXnQC77hpmIiERGKqMzsi0jCbosOVew/XPg80wcU0SkJqJY4tB80CIi6KneIiKRpR60iEhEqQctIhJRcY+HHcLXKEGLiKCHxoqIRFYUb/VWghYRQT1oEZHI0igOEZGI0igOEZGIOmhu9RYRqW1UgxYRiSjVoEVEIko9aBGRiNI4aBGRiFIPWkQkojSKQ0QkonSRUEQkoqJY4oiFHYCISBR4Nf5LxsyGmNnHZrbczK470JjUgxYRIX09aDPLAe4FBgL5wBwze8ndF1e3LSVoERHSWoPuASx395UAZvYkcC5QdxJ0yZ4CCzsGADMb5e7jw44jCnQu9tG52KeunIvq5BwzGwWMKrdpfLlz0AZYW25fPtDzQGJSDTq5UcnfctDQudhH52Kfg+5cuPt4d+9WbsnILyglaBGR9CoA2pVbbxtsqzYlaBGR9JoDdDSzY8ysPvAj4KUDaSiyNegIqfW1tTTSudhH52IfnYty3L3EzC4FXgdygAfdfdGBtGVRHJwtIiIqcYiIRJYStIhIRClBVyJdt2rWBWb2oJltNLOFYccSJjNrZ2bTzWyxmS0ys7FhxxQWM2toZrPN7IPgXNwadkx1kWrQFQhu1fyEcrdqAucdyK2adYGZ9QW2AY+4+wlhxxMWM8sD8tx9vpk1BeYBww7G74WZGdDE3beZWS7wNjDW3d8LObQ6RT3oiu29VdPd9wBlt2oelNx9JvBF2HGEzd3Xufv84PVWYAmJu8YOOp6wLVjNDRb19tJMCbpiFd2qeVD+jygVM7P2wEnArHAjCY+Z5ZjZAmAjMNXdD9pzkSlK0CLVZGaHAM8CV7j7lrDjCYu7x929K4k75XqY2UFb/soUJeiKpe1WTalbgnrrs8Dj7v5c2PFEgbsXAdOBIWHHUtcoQVcsbbdqSt0RXBh7AFji7uPCjidMZnaEmR0avG5E4oL60nCjqnuUoCvg7iVA2a2aS4BJB3qrZl1gZhOBd4FjzSzfzC4OO6aQ9AHOB043swXBcmbYQYUkD5huZh+S6NBMdfdXQo6pztEwOxGRiFIPWkQkopSgRUQiSglaRCSilKBFRCJKCVpEJKKUoOVrzCweDCFbaGZPm1njGrT1sJn9IHh9v5kdX8V7+5lZ7wM4xmoza5nq9q+8Z1tV+yt4/2/N7NfVjVHkQChBS0V2unvXYOa6PcDo8jvN7IAelebulySZ+a0fUO0ELVJXKUFLMm8BHSOmyW8AAAL8SURBVILe7Vtm9hKwOJgo504zm2NmH5rZLyBxt52Z/T2YS/t/gSPLGjKzGWbWLXg9xMzmB/MJTwsmHxoNXBn03k8N7lZ7NjjGHDPrE3z2cDN7I5iH+H7Akv0jzOwFM5sXfGbUV/bdFWyfZmZHBNu+ZWZTgs+8ZWbHVdDm5cHc0B+a2ZMHdnpFKqeHxkqlgp7yGcCUYNPJwAnuvipIcl+6e3czawD8y8zeIDHD27HA8UArYDHw4FfaPQL4H6Bv0FYLd//CzP4JbHP3PwfvewK4y93fNrOjSdzZ2Qm4BXjb3W8zs7OAVO5s/FlwjEbAHDN71t03AU2Aue5+pZndHLR9KYkHoY5292Vm1hO4Dzj9K21eBxzj7rvLbnsWSSclaKlIo2AaSUj0oB8gUXqY7e6rgu2DgBPL6stAc6Aj0BeY6O5xoNDM3qyg/V7AzLK23L2yuaYHAMcnpsAAoFkwk1xf4D+Cz75qZptT+Dddbmb/HrxuF8S6CSgFngq2PwY8FxyjN/B0uWM3qKDND4HHzewF4IUUYhCpFiVoqcjOYBrJvYJEtb38JuAyd3/9K+9L59wUMaCXu++qIJaUmVk/Esn+FHffYWYzgIaVvN2D4xZ99RxU4CwSvyyGAjeaWedgHheRtFANWg7U68Avg+k3MbNvm1kTYCYwIqhR5wGnVfDZ94C+ZnZM8NkWwfatQNNy73sDuKxsxczKEuZM4MfBtjOAw5LE2hzYHCTn40j04MvEgLK/An5MonSyBVhlZj8MjmFm1qV8g2YWA9q5+3Tg2uAYhySJQ6RalKDlQN1Por483xIPk/1vEn+RPQ8sC/Y9QmIWvP24+2fAKBLlhA/YV2J4Gfj3souEwOVAt+Ai3GL2jSa5lUSCX0Si1LEmSaxTgHpmtgS4g8QviDLbSUw2v5BEjfm2YPtPgIuD+Bbx9Uee5QCPmdlHwPvAPcG8yCJpo9nsREQiSj1oEZGIUoIWEYkoJWgRkYhSghYRiSglaBGRiFKCFhGJKCVoEZGI+n+GIKiHxIsDpAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Class 0 percentage: ', Percent(trainY2.count('1') / len(trainY2)))\n",
        "print('Class 1 percentage: ', Percent(trainY2.count('2') / len(trainY2)))\n",
        "print('Class 2 percentage: ', Percent(trainY2.count('3') / len(trainY2)))\n",
        "print('Class 3 percentage: ', Percent(trainY2.count('4') / len(trainY2)))"
      ],
      "metadata": {
        "id": "a2lL0fm0zmsY",
        "outputId": "f41ef62b-73fd-4952-f896-30ac35981b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 percentage:  29.15%\n",
            "Class 1 percentage:  19.24%\n",
            "Class 2 percentage:  22.46%\n",
            "Class 3 percentage:  29.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter5 = 1\n",
        "for i in range(len(testY)):\n",
        "  if testY[i] == '4' and y_pred[i] == '3':\n",
        "    print(str(counter5) + \". \" + testX[i])\n",
        "    counter5 += 1"
      ],
      "metadata": {
        "id": "5DPPGSfK279g",
        "outputId": "c88c803f-544d-4cf6-e61b-81ade483e5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. True love or not, the game must play out, and the fates of everyone involved, from the cast of extraordinary circus performers to the patrons, hang in the balance, suspended as precariously as the daring acrobats overhead.\n",
            "\n",
            "2. To them good ol days exist in pre industrial natural selection world or in some fictional novel.Worst ideologies are those that promises Utopian world. They roads leads to nowhere and are paved with blood.\"Nirvana is not for this world\"\n",
            "\n",
            "3. In the story, Kate Daniels is a down-on-her-luck mercenary who makes her living cleaning up these magical problems. But when Kate’s guardian is murdered, her quest for justice draws her into a power struggle between two strong factions within Atlanta’s magic\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, even though we increase the sampling of underrepresented class, we haven't seen any changes in the prediction. ***We would infer that in our case resampling cannot promote the performance of our classifier.***"
      ],
      "metadata": {
        "id": "iGO66pN59JD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3: Features Learned to most define the Class**"
      ],
      "metadata": {
        "id": "pyuYJC7Q-49v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The Strongest coefficients for each class are: \n",
        "0\t0.753\t!\n",
        "0\t0.409\ti\n",
        "0\t0.244\t?\n",
        "0\t0.232\t...\n",
        "0\t0.195\tme\n",
        "0\t0.170\tmy\n",
        "0\t0.169\t: )\n",
        "0\t0.161\t'm\n",
        "0\t0.154\twith\n",
        "0\t0.146\tna\n",
        "\n",
        "1\t0.236\tif\n",
        "1\t0.176\tIf you \n",
        "1\t0.176\tIf you\n",
        "1\t0.175\tr/52book\n",
        "1\t0.154\tliterature\n",
        "1\t0.151\tbookclub\n",
        "1\t0.147\tmaybe\n",
        "1\t0.144\tfound\n",
        "1\t0.140\tthen\n",
        "1\t0.139\thave\n",
        "\n",
        "2\t0.233\tas\n",
        "2\t0.228\tby\n",
        "2\t0.228\tbe\n",
        "2\t0.222\tr/truelit\n",
        "2\t0.210\t's\n",
        "2\t0.194\t.\n",
        "2\t0.182\tare\n",
        "2\t0.179\ton\n",
        "2\t0.167\t/r/truelit\n",
        "2\t0.164\t/r/askliterarystudies\n",
        "\n",
        "3\t0.151\tbut\n",
        "3\t0.150\t. But \n",
        "3\t0.150\t. But\n",
        "3\t0.146\tlove\n",
        "3\t0.142\tof\n",
        "3\t0.098\tone\n",
        "3\t0.093\tdark\n",
        "3\t0.091\tworld\n",
        "3\t0.087\tdiaries\n",
        "3\t0.085\ther , \n",
        "'''"
      ],
      "metadata": {
        "id": "NkzXb_XG-rSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may explain the strength of some of the features. For example, \"!\", \"?\" are signs of strong sentiment and \"me\", \"my\" are signs of personal expression, which as we defined in the guidline, are indicators of casualness. Furthermore, \"as\" can be interpreted as \"reasoning\" and make sense to be a sign of class 3. More \".\" indicate a longer text, which would more likely to have detailed explanations and thus higher probability of formalness. Although class 3 has the worst performance among all the classes, the defining features of class 3 is still quite intuitive. \"Dark\", \"world\", and \"love\" are general intangible words that aren't usually used in formal expressions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yxYC65v__6ao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "However, there're plenty of features that we wouldn't intuitively interpret their strength. \"Are\" is a singular plural present tense of \"be\" which seems very prevalent in every kinds of text in real world, but here we only have it as a strong sign of formalness. "
      ],
      "metadata": {
        "id": "qDIvUytbHJY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the features, we also see something we failed to carefully consider when selecting the data. We notice that for class 1, \"book\", \"literature\", \"r/book\" are among the top features of predicative power. This means there is a bias that the post we selected from the r/book thread are full of ordinary posts. Similarly, \"r/truelit\" and \"r/askliterary studies\" has a large proportion of formal posts, which makes them as the defining feature of class 2. This phenomenon is against our initial idea of only focusing on text itself in order to grasp the feature of casualness-formalness. We believe changing the span of posts we analyze on (also maybe remove any tags that exist in the post) can solve this issue on some extent.\n"
      ],
      "metadata": {
        "id": "x8e2Ij9IE1Cx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LogReg2 -AP4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}